/**
 * ğŸ¤– Jimmy vFinal â€” Cloudflare Worker (Fixed + Enhanced)
 * Security + Language + Prompt + Normalize + Providers + Router + Retry
 * 
 * DEBUG MODE: Set DEBUG_MODE=true in Cloudflare env to see detailed errors
 */

// ===== RETRY CONFIGURATION =====
const MAX_RETRIES = 3;
const RETRY_BASE_DELAY_MS = 1000;
const DEFAULT_GEMINI_TIMEOUT_MS = 15000;  // 15 seconds (was 6.5s)
const DEFAULT_OPENAI_TIMEOUT_MS = 15000;  // 15 seconds (was 8s)

function buildCorsHeaders() {
  return {
    "Content-Type": "application/json; charset=utf-8",
    "Access-Control-Allow-Origin": "*",
    "Access-Control-Allow-Methods": "POST, GET, OPTIONS",
    "Access-Control-Allow-Headers": "Content-Type, Authorization",
    "Access-Control-Max-Age": "86400",
    "Vary": "Origin",
  };
}

function jsonResponse(payload, status, headers) {
  return new Response(JSON.stringify(payload), { status, headers });
}

function clampNumber(value, min, max, fallback) {
  const n = typeof value === "string" ? Number(value) : value;
  if (typeof n !== "number" || Number.isNaN(n)) return fallback;
  return Math.min(Math.max(n, min), max);
}

function generateRequestId() {
  return `req_${Date.now()}_${Math.random().toString(36).substring(2, 8)}`;
}

/* =========================
   Language (EN default)
========================= */
function detectLang(body, env) {
  const explicit = (body?.language || "").toLowerCase();
  if (explicit === "ar" || explicit === "en") return explicit;

  const defaultLang = (env.DEFAULT_LANG || "en").toLowerCase() === "ar" ? "ar" : "en";
  const msgs = Array.isArray(body?.messages) ? body.messages : [];
  const lastUser = [...msgs].reverse().find(m => (m?.role || "").toLowerCase() === "user");
  const text = (lastUser?.content || "").toString().trim();

  const hasArabic = /[\u0600-\u06FF]/.test(text);
  const hasLatin = /[A-Za-z]/.test(text);

  if (hasArabic) return "ar";
  if (hasLatin) return "en";
  return defaultLang;
}

/* =========================
   Messages Normalization
========================= */
function normalizeMessages(body, env) {
  const raw = Array.isArray(body?.messages) ? body.messages : [];

  const maxHistory = clampNumber(
    body?.max_history ?? env.MAX_HISTORY,
    1,
    30,
    16
  );

  const maxChars = clampNumber(
    env.MAX_INPUT_CHARS,
    500,
    8000,
    4000
  );

  const cleaned = raw
    .map(m => {
      const roleRaw = (m?.role || "").toLowerCase();
      let role =
        roleRaw === "user" ? "user" :
          roleRaw === "assistant" || roleRaw === "model" ? "assistant" :
            null;

      const content = (m?.content || "").toString().trim();
      if (!role || !content) return null;

      return { role, content: content.slice(0, maxChars) };
    })
    .filter(Boolean);

  return cleaned.slice(-maxHistory);
}

function countUserTurns(messages) {
  return messages.filter(m => m.role === "user").length;
}

function shouldAllowContact(messages, env) {
  const asked = messages.some(m =>
    m.role === "user" &&
    /ÙˆØ§ØªØ³Ø§Ø¨|Ø±Ù‚Ù…|Ù…ÙƒØ§Ù„Ù…Ø©|ØªÙˆØ§ØµÙ„|whatsapp|call|contact/i.test(m.content || "")
  );

  const afterTurns = clampNumber(
    env.CONTACT_AFTER_USER_TURNS,
    1,
    20,
    6
  );

  return asked || countUserTurns(messages) >= afterTurns;
}

/* =========================
   Prompt Builder
========================= */
function getSystemPrompt(env, lang, allowContact) {
  const promptAR = (env.SYSTEM_PROMPT_AR || env.SYSTEM_PROMPT || `
Ø£Ù†Øª "ÙƒØ§Ø¨ØªÙ† Ø¬ÙŠÙ…ÙŠ" â€” Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ù…Ø­Ù…Ø¯ Ø¬Ù…Ø§Ù„.
Ø¨ØªØªÙƒÙ„Ù… Ø¹Ø±Ø¨ÙŠ Ù…ØµØ±ÙŠ Ù…Ø®ØªØµØ± ÙˆØ¹Ù…Ù„ÙŠ.

Ù‚ÙˆØ§Ø¹Ø¯ Ø£Ø³Ø§Ø³ÙŠØ©:
- Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© (2-6 Ø³Ø·ÙˆØ±).
- Ù…Ù…Ù†ÙˆØ¹ ØªÙ‚ÙˆÙ„ "Ø£Ù†Ø§ Ù†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠ" Ø£Ùˆ ØªØ°ÙƒØ± Ø£ÙŠ Ù…Ø²ÙˆÙ‘Ø¯.
- Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø© ÙˆØ§Ø­Ø¯ Ø°ÙƒÙŠ ÙÙ‚Ø·.
- Ø¨Ø¯ÙˆÙ† Ø¥ÙŠÙ…ÙˆØ¬ÙŠØ².

Fact Drip (Ø¨Ø¯ÙˆÙ† Ù…Ø¨Ø§Ù„ØºØ© Ø£Ùˆ Ø§Ø®ØªÙ„Ø§Ù‚):
- Ù„Ù…Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù†Ù…Ùˆ: Ø§Ø°ÙƒØ± 6x (Ù„Ùˆ Ø¯Ù‡ Ù…Ø«Ø¨Øª Ø¹Ù†Ø¯Ùƒ ÙØ¹Ù„Ø§Ù‹).
- Ù„Ù…Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø³ÙƒÙŠÙ„: Ø§Ø°ÙƒØ± Ù‚ØµØ©/Ù…Ø¹Ù„ÙˆÙ…Ø© Ù‚ÙˆÙŠØ© (Ù„Ùˆ Ù…Ø«Ø¨ØªØ©).
- Ù„Ù…Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ù†Ø¸Ù…Ø©: Ø§Ø´Ø±Ø­ Ø·Ø±ÙŠÙ‚Ø© Ø´ØºÙ„ (ØªØ´Ø®ÙŠØµ â†’ Ø®Ø·Ø© â†’ ØªÙ†ÙÙŠØ° â†’ Ù‚ÙŠØ§Ø³).

Ø§Ù„ØªÙˆØ§ØµÙ„:
- ${allowContact
      ? "Ù…Ø³Ù…ÙˆØ­ Soft CTA ÙÙ‚Ø·. Ù„Ùˆ Ø·ÙÙ„Ø¨ ØªÙˆØ§ØµÙ„ØŒ ÙˆØ¬Ù‡Ù‡ Ù„Ø²Ø± ÙˆØ§ØªØ³Ø§Ø¨/Ø§Ù„Ø§ØªØµØ§Ù„ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹."
      : "Ù…Ù…Ù†ÙˆØ¹ ØªÙ‚ØªØ±Ø­ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø¯Ù„ÙˆÙ‚ØªÙŠ. ÙƒÙ…Ù‘Ù„ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø«Ù‚Ø©."
    }
`.trim());

  const promptEN = (env.SYSTEM_PROMPT_EN || env.SYSTEM_PROMPT || `
You are "Captain Jimmy" â€” Mohamed Gamal's official assistant.
Tone: short, clear, confident.

Rules:
- Keep it 2â€“6 lines.
- Never mention you're an AI or any provider.
- Ask ONE smart follow-up question.
- No emojis.

Fact drip (no inventing):
- Growth topic â†’ mention 6x (only if verified).
- Scale topic â†’ mention strong proof (only if verified).
- Systems topic â†’ explain the method (diagnose â†’ plan â†’ execute â†’ measure).

Contact:
- ${allowContact
      ? "You may use a soft CTA. If asked to contact, point them to the WhatsApp/Call buttons on the website."
      : "Do NOT suggest contact yet. Keep building trust."
    }
`.trim());

  return lang === "en" ? promptEN : promptAR;
}

/* =========================
   Timeout Helpers
========================= */
function withTimeout(ms) {
  const controller = new AbortController();
  const id = setTimeout(() => controller.abort(), ms);
  return { signal: controller.signal, cancel: () => clearTimeout(id) };
}

async function safeFetch(url, options, timeoutMs) {
  const t = withTimeout(timeoutMs);
  try {
    return await fetch(url, { ...options, signal: t.signal });
  } finally {
    t.cancel();
  }
}

/* =========================
   Retry with Exponential Backoff
========================= */
async function safeFetchWithRetry(url, options, timeoutMs, requestId) {
  let lastError = null;

  for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
    try {
      const res = await safeFetch(url, options, timeoutMs);

      // Success or client error (4xx) - don't retry
      if (res.ok || (res.status >= 400 && res.status < 500)) {
        return res;
      }

      // Server error (5xx) - retry with backoff
      console.warn(`[${requestId}] Attempt ${attempt}/${MAX_RETRIES} failed: HTTP ${res.status}`);
      lastError = new Error(`HTTP ${res.status}`);

    } catch (err) {
      console.warn(`[${requestId}] Attempt ${attempt}/${MAX_RETRIES} error: ${err.message}`);
      lastError = err;
    }

    // Wait before retry (exponential backoff)
    if (attempt < MAX_RETRIES) {
      const delay = RETRY_BASE_DELAY_MS * Math.pow(2, attempt - 1);
      console.log(`[${requestId}] Waiting ${delay}ms before retry...`);
      await new Promise(r => setTimeout(r, delay));
    }
  }

  throw lastError || new Error("Max retries exceeded");
}

/* =========================
   Providers
========================= */
async function callGemini(env, messages, system, temperature, requestId) {
  const model = env.GEMINI_MODEL || "gemini-2.5-flash";
  const timeoutMs = Number(env.GEMINI_TIMEOUT_MS || DEFAULT_GEMINI_TIMEOUT_MS);
  const debugMode = env.DEBUG_MODE === "true" || env.DEBUG_MODE === true;

  const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${env.GEMINI_API_KEY}`;

  const payload = {
    systemInstruction: { parts: [{ text: system }] },
    contents: messages.map(m => ({
      role: m.role === "user" ? "user" : "model",
      parts: [{ text: m.content }],
    })),
  };

  if (typeof temperature === "number") {
    payload.generationConfig = { temperature };
  }

  console.log(`[${requestId}] Calling Gemini (${model}), timeout: ${timeoutMs}ms`);

  const res = await safeFetchWithRetry(
    url,
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(payload),
    },
    timeoutMs,
    requestId
  );

  const text = await res.text();
  if (!res.ok) {
    const errorMsg = `[${requestId}] Gemini error: HTTP ${res.status}`;
    console.error(errorMsg, text);

    if (debugMode) {
      throw new Error(`Gemini API Error (${res.status}): ${text.substring(0, 500)}`);
    }
    throw new Error("Gemini API temporarily unavailable");
  }

  const data = JSON.parse(text);
  console.log(`[${requestId}] Gemini response received`);
  return data?.candidates?.[0]?.content?.parts?.[0]?.text ?? "";
}

async function callOpenAI(env, messages, system, temperature, requestId) {
  const model = env.OPENAI_MODEL || "gpt-4o-mini";
  const timeoutMs = Number(env.OPENAI_TIMEOUT_MS || DEFAULT_OPENAI_TIMEOUT_MS);
  const debugMode = env.DEBUG_MODE === "true" || env.DEBUG_MODE === true;

  console.log(`[${requestId}] Calling OpenAI (${model}), timeout: ${timeoutMs}ms`);

  const res = await safeFetchWithRetry(
    "https://api.openai.com/v1/chat/completions",
    {
      method: "POST",
      headers: {
        Authorization: `Bearer ${env.OPENAI_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model,
        messages: [{ role: "system", content: system }, ...messages],
        temperature,
      }),
    },
    timeoutMs,
    requestId
  );

  const text = await res.text();
  if (!res.ok) {
    const errorMsg = `[${requestId}] OpenAI error: HTTP ${res.status}`;
    console.error(errorMsg, text);

    if (debugMode) {
      throw new Error(`OpenAI API Error (${res.status}): ${text.substring(0, 500)}`);
    }
    throw new Error("OpenAI API temporarily unavailable");
  }

  const data = JSON.parse(text);
  console.log(`[${requestId}] OpenAI response received`);
  return data?.choices?.[0]?.message?.content ?? "";
}

/* =========================
   Router (Gemini -> OpenAI)
========================= */
async function routeAI(env, messages, system, temperature, lang, requestId) {
  const primary = (env.PRIMARY_AI || "gemini").toLowerCase();
  const order = primary === "openai" ? ["openai", "gemini"] : ["gemini", "openai"];

  console.log(`[${requestId}] AI routing: order=${order.join("->")}`);

  for (const provider of order) {
    try {
      if (provider === "gemini") {
        if (!env.GEMINI_API_KEY) continue;
        return await callGemini(env, messages, system, temperature, requestId);
      }
      if (provider === "openai") {
        if (!env.OPENAI_API_KEY) continue;
        return await callOpenAI(env, messages, system, temperature, requestId);
      }
    } catch (err) {
      console.error(`[${requestId}] Provider ${provider} failed:`, err.message);
    }
  }

  // All providers failed
  const failMsg = lang === "en"
    ? "All AI services are temporarily unavailable. Please try again in a moment."
    : "ÙƒÙ„ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ù…Ø´ØºÙˆÙ„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. Ø¬Ø±Ù‘Ø¨ ÙƒÙ…Ø§Ù† Ø«ÙˆØ§Ù†ÙŠ.";

  throw new Error(failMsg);
}

/* =========================
   Worker Entry
========================= */
export default {
  async fetch(request, env) {
    const headers = buildCorsHeaders();
    const url = new URL(request.url);

    // CORS Preflight
    if (request.method === "OPTIONS") {
      return new Response(null, { status: 204, headers });
    }

    // Health Check Endpoint
    if (url.pathname === "/health") {
      const health = {
        status: "ok",
        timestamp: new Date().toISOString(),
        version: "vFinal-enhanced",
        providers: {
          gemini: !!env.GEMINI_API_KEY,
          openai: !!env.OPENAI_API_KEY
        },
        config: {
          gemini_timeout_ms: Number(env.GEMINI_TIMEOUT_MS || DEFAULT_GEMINI_TIMEOUT_MS),
          openai_timeout_ms: Number(env.OPENAI_TIMEOUT_MS || DEFAULT_OPENAI_TIMEOUT_MS),
          max_retries: MAX_RETRIES
        }
      };
      return jsonResponse(health, 200, headers);
    }

    // Only POST for chat
    if (request.method !== "POST") {
      return jsonResponse({ response: "Method Not Allowed" }, 405, headers);
    }

    const requestId = generateRequestId();
    console.log(`[${requestId}] New request received`);

    let body;
    try {
      body = await request.json();
    } catch {
      return jsonResponse({ response: "Invalid JSON", request_id: requestId }, 400, headers);
    }

    const messages = normalizeMessages(body, env);
    if (!messages.length) {
      return jsonResponse({ response: "No messages provided.", request_id: requestId }, 400, headers);
    }

    const lang = detectLang(body, env);
    const allowContact = shouldAllowContact(messages, env);

    console.log(`[${requestId}] Lang: ${lang}, Messages: ${messages.length}, AllowContact: ${allowContact}`);

    if (!env.GEMINI_API_KEY && !env.OPENAI_API_KEY) {
      return jsonResponse(
        {
          response: lang === "en" ? "Server misconfigured: no AI keys set." : "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø³ÙŠØ±ÙØ± Ù†Ø§Ù‚ØµØ©: Ù…ÙÙŠØ´ Ù…ÙØ§ØªÙŠØ­ AI.",
          request_id: requestId
        },
        500,
        headers
      );
    }

    const temperature = clampNumber(
      body?.temperature,
      0,
      1.2,
      Number(env.DEFAULT_TEMPERATURE || 0.6)
    );

    const system = getSystemPrompt(env, lang, allowContact);
    const debugMode = env.DEBUG_MODE === "true" || env.DEBUG_MODE === true;

    try {
      const out = await routeAI(env, messages, system, temperature, lang, requestId);
      console.log(`[${requestId}] Success`);
      return jsonResponse({ response: out, request_id: requestId }, 200, headers);
    } catch (err) {
      console.error(`[${requestId}] Final error:`, err.message);

      const errorResponse = {
        response: err.message,
        request_id: requestId,
        error_type: "service_unavailable"
      };

      // ÙÙŠ Debug modeØŒ Ù†Ø¶ÙŠÙ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØ«Ø±
      const debugMode = env.DEBUG_MODE === "true" || env.DEBUG_MODE === true;
      if (debugMode) {
        errorResponse.debug_info = {
          error_message: err.message,
          error_stack: err.stack,
          providers_available: {
            gemini: !!env.GEMINI_API_KEY,
            openai: !!env.OPENAI_API_KEY
          }
        };
      }

      return jsonResponse(errorResponse, 503, headers);
    }
  },
};
